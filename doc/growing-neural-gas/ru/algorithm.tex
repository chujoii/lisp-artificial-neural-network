\documentclass[unicode, 12pt, a4paper,oneside,fleqn]{article}

\input{../../../TeX/preambula-ru.tex}
\usepackage[colorlinks=true]{hyperref} % url hyperlink (beamer already include it, so move here for prevent conflict)

\author{составитель Роман В. Приходченко}

\title{Алгоритм растущего нейронного газа
  с фактором полезности нейронов}


\makeindex



\begin{document}

% меняем английские термины на русские
\renewcommand\bibname{СПИСОК ЛИТЕРАТУРЫ}
\renewcommand\refname{\centering Список литературы}
\renewcommand\contentsname{\centering Содержание}


% образцы переноса сложных слов - не работает?
% \hyphenation{веб=-ин-тер-фей-се веб-ин-тер-фей-с}
% or use in text: веб"=интерфейс (require: \usepackage[russian]{babel})

% печатаем титульный лист
\makeatletter % generate \@title, \@date, ...
\maketitle

\newpage
% печатаем оглавление
\tableofcontents

\newpage
Описание алгоритма основан на статьях:
\begin{itemize}
\item Описание алгоритма практически полностью взято из статьи в
  википедии (содержит ошибку в формуле расчёта весов, из-за чего
  нейроны разбегаются от сигнала): \cite[Нейронный
  газ]{ru.wikipedia.org},
\item \cite[A "Neural-Gas" Network Learns
  Topologies]{neural-gas-T.Martinetz-K.Schulten},
\item \cite[Растущий нейронный газ - реализация на языке
  программирования MQL5]{gng-subbotin} (содержит ошибку в описании
  коэффициента $k$, применяемого для расчёта фактора полезности)
  \item \cite[GNG-U]{gng-u-B.Fritzke}
\end{itemize}



\section{Терминология и интерпретация}
В результате работы <<расширяющегося нейронного газа с фактором
полезности>> возникают различные структуры. Далее приводится их
вольное толкование, а также базовая терминология:

\begin{itemize}
\item Входной вектор --- данные в n-мерном пространстве от изучаемого
  объекта.
\item Нейрон (узел или вершина в неориентированном графе) --- базовый
  элемент структуры искусственной нейронной сети. В нейроне хранятся
  весовые коэффициенты (вектор в n-мерном пространстве), список его
  связей, локальная ошибка и фактор полезности.
\item Связь (ребро в неориентированном графе) --- базовый элемент
  структуры. Характеризуется возрастом соединения.
\item Кластер (скопление, кисть, рой) --— объединение нескольких
  однородных элементов (нейронов) связью. Кластер может рассматриваться как
  самостоятельная единица, обладающая определёнными свойствами.
\item Два нейрона объединяются, если вектор входных данных находится в
  геометрической близости от этих двух нейронов (два нейрона являются
  победителями в сравнении расстояний от входного вектора до каждого
  нейрона).
\item Связь между нейронами разрывается если длительное время между
  этими нейронами не происходило событий (вектор входных данных не
  наблюдался в непосредственной близости от этих нейронов и возраст
  соединения превысил ограничение)
\item <<Длинные нити>> --- протяжённые, сильно вытянутые кластера.
  Образование <<длинных нитей>> может происходить в нескольких случаях:
  \begin{itemize}
  \item В процессе роста искусственной нейронной сети. После обучения
    такие <<длинные нити>> исчезают.
  \item В случае возникновения новой локации, на которую указывает
    входной вектор. Тогда в начале происходит вытягивание из
    существующего ближайшего кластера (или кластеров) --- при этом
    <<длинная нить>> становится похожей на <<щупальце>>. Если новая
    локация существует стабильно, то конец <<щупальца>> закрепляется в
    новой локации образуя новый кластер, а середина <<щупальца>>
    постепенно рассасывается. Если новая локация исчезает до фиксации
    то <<щупальце>> втягивается обратно в кластер.
  \item Также <<длинные нити>> могут образовываться в результате
    переходных процессов. В этом случае входной вектор должен
    регулярно обходить (не обязательно последовательно) всю внутреннюю
    структуру кластера.
  \end{itemize}
\end{itemize}

% читать голосом Николая Дроздова
В самом начале развития мира искусственной сети вы сможете наблюдать
зарождение простейших форм и их взаимодействие. Характерный пример:
брачные игры дождевых червей. Спустя некоторое время (которое зависит
от настроек искусственной сети и количества состояний изучаемого
объекта) --- простейшие формы кластеров эволюционируют в милых
котиков, собачек или в причудливых осьминожек. Помните искусственная
сеть всеми силами пытается стать похожей на изучаемый объект. Если вы
наблюдаете занимающихся непотребством скелетиков --- проверьте всё ли
хорошо с изучаемым объектом и настройками искусственной нейронной
сети. Записывайте свои наблюдения и они помогут раскрыть не только
богатый внутренний мир наблюдаемого объекта, но и ваш литературный
талант.



\section{Описание алгоритма <<GNG-U>>}
Модифицированный алгоритм <<Growing Neural Gas with Utility factor>>
(GNG-U).

Начиная всего с двух нейронов, алгоритм последовательно изменяет (по
большей части, увеличивает) их число, одновременно создавая набор
связей между нейронами, наилучшим образом отвечающий распределению
входных векторов. У каждого нейрона имеется внутренняя переменная, в
которой накапливается <<локальная ошибка>>. Соединения между узлами
характеризуются переменной, называемой <<возраст>>.

Каждому нейрону ставится в соответствие переменная, называемая
«фактором полезности» $U$.
\begin{enumerate}
\item \label{gng-u:1} \label{gng:1}
  Сперва создаются два узла (здесь и далее, узел=нейрон) с
  векторами весов, разрешенными распределением входных векторов, и
  нулевыми значениями локальных ошибок;

\item \label{gng-u:2} \label{gng:2}
  Узлы соединяются связью, которой можно установить возраст. На
  начальном этапе возраст равен 0.

\item \label{gng-u:3} \label{gng:3}
  Затем на вход нейросети подаётся вектор $\vec{X}$.

\item \label{gng-u:4} \label{gng:4}
  На следующем этапе находятся два нейрона $S$ и $T$, ближайших к
  $\vec{X}$ ($S$ ближе, чем $T$), то есть узлы с векторами весов
  $\vec{W_s}$ и $\vec{W_t}$, такими, что
  $\left\|\vec{X}-\vec{W_s}\right\|$ - минимальное, а
  $\left\|\vec{X}-\vec{W_t}\right\|$ — второе минимальное значение
  расстояния среди всех узлов.

\item \label{gng-u:5} \label{gng:5}
  Обновляется локальная ошибка наиболее близкого нейрона —
  победителя $S$, к ней добавляется квадрат расстояния между векторами
  $\vec{W_s}$ и $\vec{X}$.
  
  $E_{s} \Leftarrow E_{s} + \left\|\vec{X}-\vec{W_s}\right\|^{2}$

\item \label{gng-u:6} \label{gng:6}
  Эта процедура приводит к тому, что наиболее часто выигрывающие узлы,
  то есть те, в окрестности которых попадает наибольшее количество
  входных сигналов, имеют наибольшее значение локальной ошибки (можно
  считать <<локальную ошибку>> --- коэффициентом
  <<переконцентрации>>). Именно эти области становятся главными
  кандидатами на добавление новых узлов. (смотри пункт
  ~\ref{gng-u:13}--~\ref{gng-u:16})

\item \label{gng-u:7} \label{gng:7}
  Нейрон-победитель $S$ и все его топологические соседи (то есть
  все нейроны $N$, имеющие соединение с победителем) смещаются в
  сторону входного вектора на расстояния, равные долям
  $\varepsilon _w$ и $\varepsilon _n$ от полного.

  $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{X}-\vec{W_s})$
  % оригинальная формула:
  % $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{W_s}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{X}-\vec{W_n})$
  % оригинальная формула:
  % $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{W_n}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  Смещение узлов в сторону входного вектора на данном шаге означает,
  что победитель стремится <<усреднить>> своё положение среди входных
  сигналов, расположенных в его окрестностях. При этом лучший нейрон
  немного <<подтягивает>> в сторону сигнала и своих соседей.

\item \label{gng-u:8} %\label{gng:-}
  После адаптации весов нейрона-победителя мы изменяем его фактор
  полезности на величину, равную разнице между ошибкой второго лучшего
  нейрона и самого победителя:

  $U_{s} \Leftarrow U_{s} + \left\|\vec{X}-\vec{W_t}\right\|^{2} -
                            \left\|\vec{X}-\vec{W_s}\right\|^{2}$

  Физически эта добавка представляет собой величину, на которую
  изменилась бы суммарная ошибка сети, если бы нейрон-победитель в ней
  отсутствовал (тогда победителем стал бы второй лучший нейрон),
  т.е. действительно характеризует полезность нейрона для снижения
  общей ошибки.

\item \label{gng-u:9} \label{gng:8}
  Увеличить на 1 возраст всех соединений, исходящих от победителя $S$.

\item \label{gng-u:10} \label{gng:9}
  Если два лучших нейрона $S$ и $T$ соединены, обнулить возраст их
  связи. В противном случае создать связь между ними.

%\item  \label{gng-u:-} \label{gng:10}
  % Если после этого имеются нейроны, не имеющие связей с
  % другими узлами, удалить эти нейроны.

\item \label{gng-u:11} %\label{gng:-}
  Удалить все соединения, возраст которых превышает максимальный
  возраст.

  Удаляется только узел с минимальным значением полезности, причем
  только в том случае, если максимальное по слою значение локальной
  ошибки превышает его фактор полезности более чем в $k$ раз:

  $\frac{E_{max}}{U_i} > k$

  \todo[inline, caption={extinction event}]{
    \begin{minipage}{\linewidth-2cm}
      В случае возникновения <<помехи>> с входным вектором находящимся на
      очень большом расстоянии (иногда несколько десятков порядков) от
      обычных значений --- возникнет <<массовое вымирание>>. Далее я
      буду использовать термин <<среднее>> без указания дисперсии,
      считая что разброс значений локальных ошибок пренебрежимо мал по
      сравнению с космическими значениями вектора помехи. Вот как
      происходит массовое вымирание:

      \begin{itemize}
      \item При расчётах (смотри пункт~\ref{gng-u:5}) у ближайшего
        нейрона--победителя локальная ошибка $E_s$ окажется очень
        большой (опять на несколько порядков превосходящая среднее
        значение локальной ошибки остальных нейронов).

        Проблемма: локальная ошибка может слегка уменьшаться на каждом
        шаге (смотри пункт~\ref{gng-u:20}) или пополам при
        возникновении нового нейрона (смотри
        пункт~\ref{gng-u:18}). Однако деление пополам практически не
        будет встречаться, если помеха --- редкое событие.
      \item Нейрон--победитель и его соседи <<слегка>> сместятся всего
        лишь на несколько порядков в сторону помехи (смотри
        пункт~\ref{gng-u:7}). (вытягивание --- нормальный процесс,
        если помеха --- действительно одно из состояний системы)
      \item Следующим шагом будет (смотри пункт~\ref{gng-u:8}) расчёт
        фактора полезности: полезность также окажется очень большой.

        Проблемма: фактор полезности может слегка уменьшаться на
        каждом шаге (смотри пункт~\ref{gng-u:21}) а вот второго
        способа уменьшения фактора полезности в алгоритме нет.
        (Fixme: (нужно ли?) при создании нейрона для фактора
        полезности добавить пункт деления полезности (аналогичный
        пункту~\ref{gng-u:18} для локальной ошибки))
      \item Во время следующей эпохи расчёта обязательно будет удалён
        один из нейронов (смотри пункт~\ref{gng-u:11}). Причина:
        максимальная локальная ошибка $E_{max}$ на несколько порядков
        больше полезности любого нейрона $U_i$ (исключая конечно же
        нейрон выигравший помеху), а константа $k$ подобрана для
        обычного состояния сети и не рассчитана на помеху.
      \item Удаление в каждой эпохе будет продолжаться до тех пор пока
        не останется пара нейронов, ближайших к нейрону выигравшему
        помеху.
      \item Процесс добавления нейрона не будет успевать
        восстанавливать нейронную сеть: так как адаптационный шаг
        происходит только раз в $\lambda$ эпох (смотри
        пункт~\ref{gng-u:12}), а удаление случается в каждой эпохе.
      \end{itemize}

      Варианты решения проблеммы <<массового вымирания>>:
      \begin{itemize}
      \item Ничего не менять. Печально, но массовое вымирание --- один
        из этапов эволюции искусственной нейронной сети. Хорошо если
        все варианты помехи возникнут на ранней стадии формирования
        сети: потеряется не так много данных.
      \item Конечно можно решить эту проблему, просто игнорируя
        входные значения не укладывающиеся в предельно допустимые
        значения. Но что если эта помеха носит периодический характер
        и описывает ещё одно состояние системы? В таком случае
        игнорирование помехи --- плохой метод.
      \item Один из вариантов решения --- вместо максимального
        значения $E_{max}$ --- использовать медиану локальной ошибки
        $E_{median}$ (медианный фильтр достаточно прост и хорошо
        справляется с сильно неравномерными распределениями):

        $\frac{E_{median}}{U_i} > k$
      \item Другой способ --- (уже не константа) $k$ должна
        адаптироваться и к обычной сети и к возникновению
        помехи. Например рассчитываться как часть от среднего или
        медианы локальной ошибки.
      \end{itemize}
    \end{minipage}
  }

  Константа здесь играет решающее значение для способности отслеживать
  нестационарность: слишком большое ее значение приводит к более
  редким удалениям и, следовательно, снижению скорости
  адаптации. Слишком маленькое приводит к частому и многочисленному
  удалению не только действительно <<малополезных>>, но и других,
  вполне используемых нейронов.

\item \label{gng-u:12} \label{gng:11}
  Если номер текущей итерации кратен $\lambda$, и предельный
  размер сети не достигнут, создать новый нейрон $R$ по правилам. Со
  временем после нескольких циклов смещений накапливается информация,
  на основании которой принимается решение о месте, в котором должен
  быть добавлен новый нейрон. Этот процесс представляет собой
  коррекцию переменных ошибок всех нейронов слоя. Это необходимо для
  того, чтобы сеть <<забывала>> старые входные векторы и лучше
  реагировала на новые. Таким образом, достигается возможность
  использовать Расширяющийся нейронный газ для адаптации нейросети под
  нестационарные, а именно, медленно дрейфующие распределения входных
  сигналов.

\item \label{gng-u:13} \label{gng:12}
  Найти нейрон $U$ с наибольшей локальной ошибкой.

\item \label{gng-u:14} \label{gng:13}
  Среди соседей $U$ найти нейрон $V$ с максимальной ошибкой.

\item \label{gng-u:15} \label{gng:14}
  Создать узел $R$ <<посередине>> между $U$ и $V$:

  $\vec{W_r}=\frac{\vec{W_u} + \vec{W_v}}{2}$

\item \label{gng-u:16} \label{gng:15}
  Заменить связь между $U$ и $V$ на связи между $U$ и $R$, $R$ и
  $V$.

\item \label{gng-u:17} %\label{gng:-}
  при добавлении нового узла, его фактор полезности вычисляется как
  среднее арифметическое между полезностями соседних нейронов:

  $U_r \Leftarrow \frac{U_u + U_v}{2}$

\item \label{gng-u:18} \label{gng:16}
  Уменьшить ошибки нейронов $U$ и $V$, установить значение ошибки
  нейрона $R$.

  $E_u \Leftarrow E_u*a$
  
  $E_v \Leftarrow E_v*a$
  
  $E_r \Leftarrow E_u$

\item \label{gng-u:19} \label{gng:17}
  Большое значение этой ошибки служит указанием на то, что
  соответствующий нейрон лежит в области небольшого числа нейронов.

\item \label{gng-u:20} \label{gng:18}
  Уменьшить локальные ошибки всех нейронов на долю $\beta$
  
  $E_j \Leftarrow E_j - \beta*E_j$

\item \label{gng-u:21} % \label{gng:-}
  Уменьшить полезность всех нейронов на долю $\beta$

  $U_j \Leftarrow U_j - \beta*U_j$

\end{enumerate}



\section{Форма структуры данных}

Исследователь может сам задавать форму структуры кластеров, будет ли
кластеризация выполнена для гиперсферы, гипертрубы или
гиперплоскости. Если он не обладает этими знаниями, то благодаря
значению собственной ковариационной матрицы можно определить
необходимую форму. Если структура имеет хотя бы одно собственное
значение меньше выбранного пользователем порога, то модель будет
гиперлинейной, в противном случае структуру необходимо рассматривать
как нелинейное многообразие. Дальнейшая проверка покажет, имеет ли
модель форму сферы или трубы. Проверка на сферичность зависит от
выполнения неравенства $np/na > \psi$, где $np$ — это количество
векторов внутри скопления, которое находится с помощью теоремы Жордана
Брауера, а $ap$ — площадь поверхности скопления и $\psi$ — заданный
пользователем порог. Если это неравенство приобретает форму
$np/na < \psi$, то формой кластера будет <<гипертруба>>.

\section{Расстояние от вектора Х до нейронов в кластерах разной формы}

\begin{itemize}
\item Для кластера в виде гипертрубы рассчитывается радиальная мера
  расстояния:

  $d^2_{i,j} = \frac
  {(\left\|\vec{x^t_i}-\vec{v_j}\right\|A_j - 1)^2 \left\|\vec{x^t_i}-\vec{v_j}\right\|^2}
  {\left\|\vec{x^t_i}-\vec{v_j}\right\|^2A_j}$
  
  где: \begin{itemize}
  \item $A_j$ — это положительная, определённой матрица, посчитанная
    для учёта эксцентриситета и ориентации гипертрубы. Значение $A_j$ для
    этого уравнения находится с помощью гиперлипсоида Лоунера, используя
    алгоритм Хачияна.
  \end{itemize}

\item Для определения расстояний в гиперплоскости следует использовать
  следующую формулу

  $d^2_{i,j} = 
  \left\|\vec{x^t_i}-\vec{v_j}\right\|^2A -
  \sum_{k=1}^{p} {<\vec{x^t_i}-\vec{v_j}, \vec{b_{j,k}}>^2A}$

  \todo{неясный верхний предел суммы}
  \todo{странные угловые скобочки что-то символизируют?}
  
  где: \begin{itemize}
  \item $A_j$, это сколь угодно позитивно определённая симметричная
    матрица весов. \todo{в формуле фигурирует просто $A$ без инедксов}
  \item $b_{j, k}$ оценивается с помощью нахождения собственных
    векторов нейронных узлов модели.
    \end{itemize}

  \item Для определения расстояния в гиперсфере необходимо
    использовать формулу
    
    $d^2_{i,j} = \left\|\vec{x^t_i}-\vec{v_j}\right\|^2$
    
    где: \begin{itemize}
    \item $w_i$ — либо среднее значение векторов, заключённых в
      плоскости. \todo{$w_i$ отсутствует в приведённой формуле}
    \end{itemize}
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{algorithm}

\end{document}
