\documentclass[unicode, 12pt, a4paper,oneside,fleqn]{article}

\input{../../../TeX/preambula-ru.tex}
\usepackage[colorlinks=true]{hyperref} % url hyperlink (beamer already include it, so move here for prevent conflict)

\author{Роман В. Приходченко}

\title{Алгоритм растущего нейронного газа}


\makeindex



\begin{document}

% меняем английские термины на русские
\renewcommand\bibname{СПИСОК ЛИТЕРАТУРЫ}
\renewcommand\refname{\centering Список литературы}
\renewcommand\contentsname{\centering Содержание}


% образцы переноса сложных слов - не работает?
% \hyphenation{веб=-ин-тер-фей-се веб-ин-тер-фей-с}
% or use in text: веб"=интерфейс (require: \usepackage[russian]{babel})

% печатаем титульный лист
\makeatletter % generate \@title, \@date, ...
\maketitle

\newpage
% печатаем оглавление
\tableofcontents

\newpage
Описание алгоритма основан на статьях:
\begin{itemize}
\item Описание алгоритма практически полностью взято из статьи в
  википедии: \cite[Нейронный газ]{ru.wikipedia.org},
\item \cite[A "Neural-Gas" Network Learns
  Topologies]{neural-gas-T.Martinetz-K.Schulten},
\item \cite[Растущий нейронный газ - реализация на языке
  программирования MQL5]{gng-subbotin} приведённых в списке
  литературы.
\end{itemize}

\section{Описание алгоритма}
Начиная всего с двух нейронов, алгоритм последовательно изменяет (по
большей части, увеличивает) их число, одновременно создавая набор
связей между нейронами, наилучшим образом отвечающий распределению
входных векторов. У каждого нейрона имеется внутренняя переменная, в
которой накапливается <<локальная ошибка>>. Соединения между узлами
характеризуются переменной, называемой <<возраст>>.

\begin{enumerate}
\item Сперва создаются два узла (здесь и далее, узел=нейрон) с
  векторами весов, разрешенными распределением входных векторов, и
  нулевыми значениями локальных ошибок;
  
\item Узлы соединяются связью, которой можно установить возраст. На
  начальном этапе возраст равен 0.
  
\item Затем на вход нейросети подаётся вектор $\vec{X}$.

\item На следующем этапе находятся два нейрона $S$ и $T$, ближайших к
  $\vec{X}$ ($S$ ближе, чем $T$), то есть узлы с векторами весов
  $\vec{W_s}$ и $\vec{W_t}$, такими, что
  $\left\|\vec{W_t}-\vec{X}\right\|$ - минимальное, а
  $\left\|\vec{W_t}-\vec{X}\right\|$ — второе минимальное значение
  расстояния среди всех узлов.
  
\item Обновляется локальная ошибка наиболее близкого нейрона —
  победителя $S$, к ней добавляется квадрат расстояния между векторами
  $\vec{W_s}$ и $\vec{X}$.
  
  $E_{s}\Rightarrow E_{s} + \left\|\vec{W_s}-\vec {X}\right\|^{2}$

\item Эта процедура приводит к тому, что наиболее часто выигрывающие
  узлы, то есть те, в окрестности которых попадает наибольшее
  количество входных сигналов, имеют наибольшее значение ошибки, а
  значит, именно эти области становятся главными кандидатами на
  <<уплотнение>> путём добавления новых узлов.

\item Нейрон-победитель $S$ и все его топологические соседи (то есть
  все нейроны $N$, имеющие соединение с победителем) смещаются в
  сторону входного вектора на расстояния, равные долям
  $\varepsilon _w$ и $\varepsilon _n$ от полного.

  $\vec{W_s}\Rightarrow \vec{W_s}+\varepsilon_w(\vec{X}-\vec{W_s})$
  % оригинальная формула:
  % $\vec{W_s}\Rightarrow \vec{W_s}+\varepsilon_w(\vec{W_s}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  $\vec{W_n}\Rightarrow \vec{W_n}+\varepsilon_n(\vec{X}-\vec{W_n})$
  % оригинальная формула:
  % $\vec{W_n}\Rightarrow \vec{W_n}+\varepsilon_n(\vec{W_n}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  Смещение узлов в сторону входного вектора на данном шаге означает,
  что победитель стремится <<усреднить>> своё положение среди входных
  сигналов, расположенных в его окрестностях. При этом лучший нейрон
  немного <<подтягивает>> в сторону сигнала и своих соседей.

\item Увеличить на 1 возраст всех соединений, исходящих от победителя $S$.

\item Если два лучших нейрона $S$ и $T$ соединены, обнулить возраст их
  связи. В противном случае создать связь между ними.

\item Удалить все соединения, возраст которых превышает максимальный
  возраст. Если после этого имеются нейроны, не имеющие связей с
  другими узлами, удалить эти нейроны.

\item Если номер текущей итерации кратен $\lambda$, и предельный
  размер сети не достигнут, создать новый нейрон $R$ по правилам. Со
  временем после нескольких циклов смещений накапливается информация,
  на основании которой принимается решение о месте, в котором должен
  быть добавлен новый нейрон. Этот процесс представляет собой
  коррекцию переменных ошибок всех нейронов слоя. Это необходимо для
  того, чтобы сеть <<забывала>> старые входные векторы и лучше
  реагировала на новые. Таким образом, достигается возможность
  использовать Расширяющийся нейронный газ для адаптации нейросети под
  нестационарные, а именно, медленно дрейфующие распределения входных
  сигналов.

\item Найти нейрон $U$ с наибольшей локальной ошибкой.

\item Среди соседей $U$ найти нейрон $V$ с максимальной ошибкой.

\item Создать узел $R$ <<посередине>> между $U$ и $V$:

  $\vec{W_r}=\frac{\vec{W_u} + \vec{W_v}}{2}$

\item Заменить связь между $U$ и $V$ на связи между $U$ и $R$, $R$ и
  $V$.
  
\item Уменьшить ошибки нейронов $U$ и $V$, установить значение ошибки
  нейрона $R$.

  $E_u\Rightarrow E_u*a$
  
  $E_v\Rightarrow E_v*a$
  
  $E_r\Rightarrow E_u$

\item Большое значение этой ошибки служит указанием на то, что
  соответствующий нейрон лежит в области небольшого числа нейронов.

\item Каждый раз, когда для случайно выбранного $X$ определяется ближайший к нему нейрон $\vec{W_j}$, локальная ошибка для последнего $E_j$ получает приращение $\left\|\vec{W_j}-\vec{X}\right\|^{2}$.
  % повтор пункта 5?
\end{enumerate}
  
\section{Форма структуры данных}

Исследователь может сам задавать форму структуры кластеров, будет ли
кластеризация выполнена для гиперсферы, гипертрубы или
гиперплоскости. Если он не обладает этими знаниями, то благодаря
значению собственной ковариационной матрицы можно определить
необходимую форму. Если структура имеет хотя бы одно собственное
значение меньше выбранного пользователем порога, то модель будет
гиперлинейной, в противном случае структуру необходимо рассматривать
как нелинейное многообразие. Дальнейшая проверка покажет, имеет ли
модель форму сферы или трубы. Проверка на сферичность зависит от
выполнения неравенства $np/na > \psi$, где $np$ — это количество
векторов внутри скопления, которое находится с помощью теоремы Жордана
Брауера, а $ap$ — площадь поверхности скопления и $\psi$ — заданный
пользователем порог. Если это неравенство приобретает форму
$np/na < \psi$, то формой кластера будет <<гипертруба>>.

\section{Расстояние от вектора Х до нейронов в кластерах разной формы}

\begin{itemize}
\item Для кластера в виде гипертрубы рассчитывается радиальная мера
  расстояния:

  $d^2_{i,j} = \frac
  {(\left\|\vec{x^t_i}-\vec{v_j}\right\|A_j - 1)^2 \left\|\vec{x^t_i}-\vec{v_j}\right\|^2}
  {\left\|\vec{x^t_i}-\vec{v_j}\right\|^2A_j}$
  
  где: \begin{itemize}
  \item $A_j$ — это положительная, определённой матрица, посчитанная
    для учёта эксцентриситета и ориентации гипертрубы. Значение $A_j$ для
    этого уравнения находится с помощью гиперлипсоида Лоунера, используя
    алгоритм Хачияна.
  \end{itemize}

\item Для определения расстояний в гиперплоскости следует использовать
  следующую формулу

  $d^2_{i,j} = 
  \left\|\vec{x^t_i}-\vec{v_j}\right\|^2A -
  \sum_{k=1}^{p} {<\vec{x^t_i}-\vec{v_j}, \vec{b_{j,k}}>^2A}$

  \todo{неясный верхний предел суммы}
  \todo{странные угловые скобочки что-то символизируют?}
  
  где: \begin{itemize}
  \item $A_j$, это сколь угодно позитивно определённая симметричная
    матрица весов. \todo{в формуле фигурирует просто $A$ без инедксов}
  \item $b_{j, k}$ оценивается с помощью нахождения собственных
    векторов нейронных узлов модели.
    \end{itemize}

  \item Для определения расстояния в гиперсфере необходимо
    использовать формулу
    
    $d^2_{i,j} = \left\|\vec{x^t_i}-\vec{v_j}\right\|^2$
    
    где: \begin{itemize}
    \item $w_i$ — либо среднее значение векторов, заключённых в
      плоскости. \todo{$w_i$ отсутствует в приведённой формуле}
    \end{itemize}
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{algorithm}

\end{document}
