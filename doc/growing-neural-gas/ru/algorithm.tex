\documentclass[unicode, 12pt, a4paper,oneside,fleqn]{article}

\input{../../../TeX/preambula-ru.tex}
\usepackage[colorlinks=true]{hyperref} % url hyperlink (beamer already include it, so move here for prevent conflict)

\author{составитель Роман В. Приходченко}

\title{Алгоритм растущего нейронного газа
  с фактором полезности нейронов}


\makeindex



\begin{document}

% меняем английские термины на русские
\renewcommand\bibname{СПИСОК ЛИТЕРАТУРЫ}
\renewcommand\refname{\centering Список литературы}
\renewcommand\contentsname{\centering Содержание}


% образцы переноса сложных слов - не работает?
% \hyphenation{веб=-ин-тер-фей-се веб-ин-тер-фей-с}
% or use in text: веб"=интерфейс (require: \usepackage[russian]{babel})

% печатаем титульный лист
\makeatletter % generate \@title, \@date, ...
\maketitle

\newpage
% печатаем оглавление
\tableofcontents

\newpage
Описание алгоритма основан на статьях:
\begin{itemize}
\item Описание алгоритма практически полностью взято из статьи в
  википедии: \cite[Нейронный газ]{ru.wikipedia.org},
\item \cite[A "Neural-Gas" Network Learns
  Topologies]{neural-gas-T.Martinetz-K.Schulten},
\item \cite[Растущий нейронный газ - реализация на языке
  программирования MQL5]{gng-subbotin} приведённых в списке
  литературы.
\end{itemize}

\section{Описание алгоритма <<GNG-U>>}
Модифицированный алгоритм <<Growing Neural Gas with Utility factor>>
(GNG-U).

Начиная всего с двух нейронов, алгоритм последовательно изменяет (по
большей части, увеличивает) их число, одновременно создавая набор
связей между нейронами, наилучшим образом отвечающий распределению
входных векторов. У каждого нейрона имеется внутренняя переменная, в
которой накапливается <<локальная ошибка>>. Соединения между узлами
характеризуются переменной, называемой <<возраст>>.

Каждому нейрону ставится в соответствие переменная, называемая
«фактором полезности» $U$.
\begin{enumerate}
  % in round braces (GNG-U); in square braces [GNG]
\item % (1) [1]
  Сперва создаются два узла (здесь и далее, узел=нейрон) с
  векторами весов, разрешенными распределением входных векторов, и
  нулевыми значениями локальных ошибок;

\item % (2) [2]
  Узлы соединяются связью, которой можно установить возраст. На
  начальном этапе возраст равен 0.

\item % (2) [3]
  Затем на вход нейросети подаётся вектор $\vec{X}$.

\item % (4) [4]
  На следующем этапе находятся два нейрона $S$ и $T$, ближайших к
  $\vec{X}$ ($S$ ближе, чем $T$), то есть узлы с векторами весов
  $\vec{W_s}$ и $\vec{W_t}$, такими, что
  $\left\|\vec{X}-\vec{W_s}\right\|$ - минимальное, а
  $\left\|\vec{X}-\vec{W_t}\right\|$ — второе минимальное значение
  расстояния среди всех узлов.

\item % (5) [5]
  Обновляется локальная ошибка наиболее близкого нейрона —
  победителя $S$, к ней добавляется квадрат расстояния между векторами
  $\vec{W_s}$ и $\vec{X}$.
  
  $E_{s} \Leftarrow E_{s} + \left\|\vec{X}-\vec{W_s}\right\|^{2}$

\item % (6) [6]
  Эта процедура приводит к тому, что наиболее часто выигрывающие
  узлы, то есть те, в окрестности которых попадает наибольшее
  количество входных сигналов, имеют наибольшее значение ошибки, а
  значит, именно эти области становятся главными кандидатами на
  <<уплотнение>> путём добавления новых узлов.

\item % (7) [7]
  Нейрон-победитель $S$ и все его топологические соседи (то есть
  все нейроны $N$, имеющие соединение с победителем) смещаются в
  сторону входного вектора на расстояния, равные долям
  $\varepsilon _w$ и $\varepsilon _n$ от полного.

  $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{X}-\vec{W_s})$
  % оригинальная формула:
  % $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{W_s}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{X}-\vec{W_n})$
  % оригинальная формула:
  % $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{W_n}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  Смещение узлов в сторону входного вектора на данном шаге означает,
  что победитель стремится <<усреднить>> своё положение среди входных
  сигналов, расположенных в его окрестностях. При этом лучший нейрон
  немного <<подтягивает>> в сторону сигнала и своих соседей.

\item % (8) [-]
  После адаптации весов нейрона-победителя мы изменяем его фактор
  полезности на величину, равную разнице между ошибкой второго лучшего
  нейрона и самого победителя:

  $U_{s} \Leftarrow U_{s} + \left\|\vec{X}-\vec{W_t}\right\|^{2} -
                            \left\|\vec{X}-\vec{W_s}\right\|^{2}$

  Физически эта добавка представляет собой величину, на которую
  изменилась бы суммарная ошибка сети, если бы нейрон-победитель в ней
  отсутствовал (тогда победителем стал бы второй лучший нейрон),
  т.е. действительно характеризует полезность нейрона для снижения
  общей ошибки.

\item % (9) [8]
  Увеличить на 1 возраст всех соединений, исходящих от победителя $S$.

\item % (10) [9]
  Если два лучших нейрона $S$ и $T$ соединены, обнулить возраст их
  связи. В противном случае создать связь между ними.

\item % (11) [10]
  Удалить все соединения, возраст которых превышает максимальный
  возраст.

  % [GNG]:
  % Если после этого имеются нейроны, не имеющие связей с
  % другими узлами, удалить эти нейроны.

  Удаляется только узел с минимальным значением полезности, причем
  только в том случае, если максимальное по слою значение локальной
  ошибки превышает его фактор полезности более чем в $k$ раз:

  $\frac{E_{max}}{U_i} > k$

  Константа здесь играет решающее значение для способности отслеживать
  нестационарность: слишком большое ее значение приводит к удалению не
  только действительно <<малополезных>>, но и других, вполне
  используемых нейронов, слишком маленькое – к более редким удалениям
  и, следовательно, снижению скорости адаптации.

\item % (12) [11]
  Если номер текущей итерации кратен $\lambda$, и предельный
  размер сети не достигнут, создать новый нейрон $R$ по правилам. Со
  временем после нескольких циклов смещений накапливается информация,
  на основании которой принимается решение о месте, в котором должен
  быть добавлен новый нейрон. Этот процесс представляет собой
  коррекцию переменных ошибок всех нейронов слоя. Это необходимо для
  того, чтобы сеть <<забывала>> старые входные векторы и лучше
  реагировала на новые. Таким образом, достигается возможность
  использовать Расширяющийся нейронный газ для адаптации нейросети под
  нестационарные, а именно, медленно дрейфующие распределения входных
  сигналов.

\item % (13) [12]
  Найти нейрон $U$ с наибольшей локальной ошибкой.

\item % (14) [13]
  Среди соседей $U$ найти нейрон $V$ с максимальной ошибкой.

\item % (15) [14]
  Создать узел $R$ <<посередине>> между $U$ и $V$:

  $\vec{W_r}=\frac{\vec{W_u} + \vec{W_v}}{2}$

\item % (16) [15]
  Заменить связь между $U$ и $V$ на связи между $U$ и $R$, $R$ и
  $V$.

\item % (17) [-]
  при добавлении нового узла, его фактор полезности вычисляется как
  среднее арифметическое между полезностями соседних нейронов:

  $U_r \Leftarrow \frac{U_u + U_v}{2}$

\item % (18) [16]
  Уменьшить ошибки нейронов $U$ и $V$, установить значение ошибки
  нейрона $R$.

  $E_u \Leftarrow E_u*a$
  
  $E_v \Leftarrow E_v*a$
  
  $E_r \Leftarrow E_u$

\item % (19) [17]
  Большое значение этой ошибки служит указанием на то, что
  соответствующий нейрон лежит в области небольшого числа нейронов.

\item % (20) [18]
  Уменьшить локальные ошибки всех нейронов на долю $\beta$
  
  $E_j \Leftarrow E_j - \beta*E_j$

  \todo{мне кажется если за $\beta$ принимать не $0.0001$, а считать
    что $\beta = 0.9999$, то формула сильно упростится не потеряв
    смысла: $E_j \Leftarrow \beta*E_j$}

\item % (21) [-]
  Уменьшить полезность всех нейронов на долю $\beta$

  $U_j \Leftarrow U_j - \beta*U_j$

  \todo{мне кажется если за $\beta$ принимать не $0.0001$, а считать
    что $\beta = 0.9999$, то формула сильно упростится не потеряв
    смысла: $U_j \Leftarrow \beta*U_j$}
\end{enumerate}
  
\section{Форма структуры данных}

Исследователь может сам задавать форму структуры кластеров, будет ли
кластеризация выполнена для гиперсферы, гипертрубы или
гиперплоскости. Если он не обладает этими знаниями, то благодаря
значению собственной ковариационной матрицы можно определить
необходимую форму. Если структура имеет хотя бы одно собственное
значение меньше выбранного пользователем порога, то модель будет
гиперлинейной, в противном случае структуру необходимо рассматривать
как нелинейное многообразие. Дальнейшая проверка покажет, имеет ли
модель форму сферы или трубы. Проверка на сферичность зависит от
выполнения неравенства $np/na > \psi$, где $np$ — это количество
векторов внутри скопления, которое находится с помощью теоремы Жордана
Брауера, а $ap$ — площадь поверхности скопления и $\psi$ — заданный
пользователем порог. Если это неравенство приобретает форму
$np/na < \psi$, то формой кластера будет <<гипертруба>>.

\section{Расстояние от вектора Х до нейронов в кластерах разной формы}

\begin{itemize}
\item Для кластера в виде гипертрубы рассчитывается радиальная мера
  расстояния:

  $d^2_{i,j} = \frac
  {(\left\|\vec{x^t_i}-\vec{v_j}\right\|A_j - 1)^2 \left\|\vec{x^t_i}-\vec{v_j}\right\|^2}
  {\left\|\vec{x^t_i}-\vec{v_j}\right\|^2A_j}$
  
  где: \begin{itemize}
  \item $A_j$ — это положительная, определённой матрица, посчитанная
    для учёта эксцентриситета и ориентации гипертрубы. Значение $A_j$ для
    этого уравнения находится с помощью гиперлипсоида Лоунера, используя
    алгоритм Хачияна.
  \end{itemize}

\item Для определения расстояний в гиперплоскости следует использовать
  следующую формулу

  $d^2_{i,j} = 
  \left\|\vec{x^t_i}-\vec{v_j}\right\|^2A -
  \sum_{k=1}^{p} {<\vec{x^t_i}-\vec{v_j}, \vec{b_{j,k}}>^2A}$

  \todo{неясный верхний предел суммы}
  \todo{странные угловые скобочки что-то символизируют?}
  
  где: \begin{itemize}
  \item $A_j$, это сколь угодно позитивно определённая симметричная
    матрица весов. \todo{в формуле фигурирует просто $A$ без инедксов}
  \item $b_{j, k}$ оценивается с помощью нахождения собственных
    векторов нейронных узлов модели.
    \end{itemize}

  \item Для определения расстояния в гиперсфере необходимо
    использовать формулу
    
    $d^2_{i,j} = \left\|\vec{x^t_i}-\vec{v_j}\right\|^2$
    
    где: \begin{itemize}
    \item $w_i$ — либо среднее значение векторов, заключённых в
      плоскости. \todo{$w_i$ отсутствует в приведённой формуле}
    \end{itemize}
\end{itemize}

\newpage
\bibliographystyle{plain}
\bibliography{algorithm}

\end{document}
