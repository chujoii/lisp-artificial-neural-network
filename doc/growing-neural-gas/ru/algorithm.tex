\documentclass[unicode, 12pt, a4paper,oneside,fleqn]{article}

\input{../../../TeX/preambula-ru.tex}
\usepackage[colorlinks=true]{hyperref} % url hyperlink (beamer already include it, so move here for prevent conflict)

\author{составитель Роман В. Приходченко}

\title{Алгоритм растущего нейронного газа
  с фактором полезности нейронов}


\makeindex



\begin{document}

% меняем английские термины на русские
\renewcommand\bibname{СПИСОК ЛИТЕРАТУРЫ}
\renewcommand\refname{\centering Список литературы}
\renewcommand\contentsname{\centering Содержание}


% образцы переноса сложных слов - не работает?
% \hyphenation{веб=-ин-тер-фей-се веб-ин-тер-фей-с}
% or use in text: веб"=интерфейс (require: \usepackage[russian]{babel})

% печатаем титульный лист
\makeatletter % generate \@title, \@date, ...
\maketitle

\begin{table}[ht]
  \begin{tabular}{cc}
    \includegraphics[width=2cm]{../../../CC_BY-SA_88x31.png} &
    \shortstack{руководство распространяется в соответствии с
      условиями\\
      \href{http://creativecommons.org/licenses/by-sa/3.0/}{Attribution-ShareAlike} \\
      (Атрибуция — С сохранением условий) CC BY-SA \\
      Копирование и распространение приветствуется.}
  \end{tabular}
\end{table}

\newpage
% печатаем оглавление
\tableofcontents

\newpage
Описание алгоритма основан на статьях:
\begin{itemize}
\item Описание алгоритма практически полностью взято из статьи в
  википедии (содержит ошибку в формуле расчёта весов, из-за чего
  нейроны разбегаются от сигнала): \cite[Нейронный
  газ]{ru.wikipedia.org},
\item \cite[A "Neural-Gas" Network Learns
  Topologies]{neural-gas-T.Martinetz-K.Schulten},
\item \cite[Растущий нейронный газ - реализация на языке
  программирования MQL5]{gng-subbotin} (содержит ошибку в описании
  коэффициента $k$, применяемого для расчёта фактора полезности)
  \item \cite[GNG-U]{gng-u-B.Fritzke}
\end{itemize}



\section{Терминология и интерпретация}
В результате работы <<расширяющегося нейронного газа с фактором
полезности>> возникают различные структуры. Далее приводится их
вольное толкование, а также базовая терминология:

\begin{itemize}
\item Входной вектор --- данные в n-мерном пространстве от изучаемого
  объекта.
\item Нейрон (узел или вершина в неориентированном графе) --- базовый
  элемент структуры искусственной нейронной сети. В нейроне хранятся
  весовые коэффициенты (вектор в n-мерном пространстве), список его
  связей, локальная ошибка и фактор полезности.
\item Связь (ребро в неориентированном графе) --- базовый элемент
  структуры. Характеризуется возрастом соединения.
\item Кластер (скопление, кисть, рой) --— объединение нескольких
  однородных элементов (нейронов) связью. Кластер может рассматриваться как
  самостоятельная единица, обладающая определёнными свойствами.
\item Два нейрона объединяются, если вектор входных данных находится в
  геометрической близости от этих двух нейронов (два нейрона являются
  победителями в сравнении расстояний от входного вектора до каждого
  нейрона).
\item Связь между нейронами разрывается, если длительное время между
  этими нейронами не происходило событий (вектор входных данных не
  наблюдался в непосредственной близости от соединения этих нейронов и
  возраст соединения превысил ограничение)
\item <<Длинные нити>> --- протяжённые, сильно вытянутые кластера.
  Образование <<длинных нитей>> может происходить в нескольких случаях:
  \begin{itemize}
  \item В процессе роста искусственной нейронной сети. После обучения
    такие <<длинные нити>> исчезают.
  \item В случае возникновения новой локации, на которую указывает
    входной вектор. Тогда в начале происходит вытягивание из
    существующего ближайшего кластера (или кластеров) --- при этом
    <<длинная нить>> становится похожей на <<щупальце>>. Если новая
    локация существует стабильно, то конец <<щупальца>> закрепляется в
    новой локации образуя новый кластер, а середина <<щупальца>>
    постепенно рассасывается. Если новая локация исчезает до фиксации
    то <<щупальце>> втягивается обратно в кластер.
  \item Также <<длинные нити>> могут образовываться в результате
    переходных процессов. В этом случае входной вектор должен
    регулярно обходить (не обязательно последовательно) всю внутреннюю
    структуру кластера.
  \end{itemize}
\end{itemize}

% читать голосом Николая Дроздова
В самом начале развития мира искусственной сети вы сможете наблюдать
зарождение простейших форм и их взаимодействие. Характерный пример:
брачные игры дождевых червей. Спустя некоторое время (которое зависит
от настроек искусственной сети и количества состояний изучаемого
объекта) --- простейшие формы кластеров эволюционируют в милых
котиков, собачек или в причудливых осьминожек. Помните искусственная
сеть всеми силами пытается стать похожей на изучаемый объект. Если вы
наблюдаете занимающихся непотребством скелетиков --- проверьте всё ли
хорошо с изучаемым объектом и настройками искусственной нейронной
сети. Записывайте свои наблюдения и они помогут раскрыть не только
богатый внутренний мир наблюдаемого объекта, но и ваш литературный
талант.



\section{Описание алгоритма <<GNG-U>>}
Модифицированный алгоритм <<Growing Neural Gas with Utility factor>>
(GNG-U).

Начиная всего с двух нейронов, алгоритм последовательно изменяет (по
большей части, увеличивает) их число, одновременно создавая набор
связей между нейронами, наилучшим образом отвечающий распределению
входных векторов. У каждого нейрона имеется внутренняя переменная, в
которой накапливается <<локальная ошибка>>. Соединения между узлами
характеризуются переменной, называемой <<возраст>>.

Каждому нейрону ставится в соответствие переменная, называемая
«фактором полезности» $U$.
\begin{enumerate}
\item \label{gng-u:1} \label{gng:1}
  Сперва создаются два узла (здесь и далее, узел=нейрон) с
  векторами весов, разрешенными распределением входных векторов, и
  нулевыми значениями локальных ошибок;

\item \label{gng-u:2} \label{gng:2}
  Узлы соединяются связью, которой можно установить возраст. На
  начальном этапе возраст равен 0.

\item \label{gng-u:3} \label{gng:3}
  Затем на вход нейросети подаётся вектор $\vec{X}$.

\item \label{gng-u:4} \label{gng:4}
  На следующем этапе находятся два нейрона $S$ и $T$, ближайших к
  $\vec{X}$ ($S$ ближе, чем $T$), то есть узлы с векторами весов
  $\vec{W_s}$ и $\vec{W_t}$, такими, что
  $\left\|\vec{X}-\vec{W_s}\right\|$ - минимальное, а
  $\left\|\vec{X}-\vec{W_t}\right\|$ — второе минимальное значение
  расстояния среди всех узлов.

\item \label{gng-u:5} \label{gng:5}
  Обновляется локальная ошибка наиболее близкого нейрона —
  победителя $S$, к ней добавляется квадрат расстояния между векторами
  $\vec{W_s}$ и $\vec{X}$.
  
  $E_{s} \Leftarrow E_{s} + \left\|\vec{X}-\vec{W_s}\right\|^{2}$

\item \label{gng-u:6} \label{gng:6}
  Эта процедура приводит к тому, что наиболее часто выигрывающие узлы,
  то есть те, в окрестности которых попадает наибольшее количество
  входных сигналов, имеют наибольшее значение локальной ошибки (можно
  считать <<локальную ошибку>> --- коэффициентом
  <<переконцентрации>>). Именно эти области становятся главными
  кандидатами на добавление новых узлов. (смотри пункт
  ~\ref{gng-u:13}--~\ref{gng-u:16})

\item \label{gng-u:7} \label{gng:7}
  Нейрон-победитель $S$ и все его топологические соседи (то есть
  все нейроны $N$, имеющие соединение с победителем) смещаются в
  сторону входного вектора на расстояния, равные долям
  $\varepsilon _w$ и $\varepsilon _n$ от полного.

  $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{X}-\vec{W_s})$
  % оригинальная формула:
  % $\vec{W_s} \Leftarrow \vec{W_s}+\varepsilon_w(\vec{W_s}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{X}-\vec{W_n})$
  % оригинальная формула:
  % $\vec{W_n} \Leftarrow \vec{W_n}+\varepsilon_n(\vec{W_n}-\vec{X})$
  % либо varepsilon_w отрицательна, либо порядок вычитания неправильный

  Смещение узлов в сторону входного вектора на данном шаге означает,
  что победитель стремится <<усреднить>> своё положение среди входных
  сигналов, расположенных в его окрестностях. При этом лучший нейрон
  немного <<подтягивает>> в сторону сигнала и своих соседей.

\item \label{gng-u:8} %\label{gng:-}
  После адаптации весов нейрона-победителя мы изменяем его фактор
  полезности на величину, равную разнице между ошибкой второго лучшего
  нейрона и самого победителя:

  $U_{s} \Leftarrow U_{s} + \left\|\vec{X}-\vec{W_t}\right\|^{2} -
                            \left\|\vec{X}-\vec{W_s}\right\|^{2}$

  Физически эта добавка представляет собой величину, на которую
  изменилась бы суммарная ошибка сети, если бы нейрон-победитель в ней
  отсутствовал (тогда победителем стал бы второй лучший нейрон),
  т.е. действительно характеризует полезность нейрона для снижения
  общей ошибки.

\item \label{gng-u:9} \label{gng:8}
  Увеличить на 1 возраст всех соединений, исходящих от победителя $S$.

\item \label{gng-u:10} \label{gng:9}
  Если два лучших нейрона $S$ и $T$ соединены, обнулить возраст их
  связи. В противном случае создать связь между ними.

%\item  \label{gng-u:-} \label{gng:10}
  % Если после этого имеются нейроны, не имеющие связей с
  % другими узлами, удалить эти нейроны.

\item \label{gng-u:11} %\label{gng:-}
  Удалить все соединения, возраст которых превышает максимальный
  возраст.

  Удаляется только узел с минимальным значением полезности, причем
  только в том случае, если максимальное по слою значение локальной
  ошибки превышает его фактор полезности более чем в $k$ раз:

  $\frac{E_{max}}{U_i} > k$

  \todo[inline, caption={extinction event}]{
    \begin{minipage}{\linewidth-2cm}
      В случае возникновения <<помехи>> с входным вектором находящимся на
      очень большом расстоянии (иногда несколько десятков порядков) от
      обычных значений --- возникнет <<массовое вымирание>>. Далее я
      буду использовать термин <<среднее>> без указания дисперсии,
      считая что разброс значений локальных ошибок пренебрежимо мал по
      сравнению с космическими значениями вектора помехи. Вот как
      происходит массовое вымирание:

      \begin{itemize}
      \item При расчётах (смотри пункт~\ref{gng-u:5}) у ближайшего
        нейрона--победителя локальная ошибка $E_s$ окажется очень
        большой (опять на несколько порядков превосходящая среднее
        значение локальной ошибки остальных нейронов).

        Проблемма: локальная ошибка может слегка уменьшаться на каждом
        шаге (смотри пункт~\ref{gng-u:20}) или пополам при
        возникновении нового нейрона (смотри
        пункт~\ref{gng-u:18}). Однако деление пополам практически не
        будет встречаться, если помеха --- редкое событие.
      \item Нейрон--победитель и его соседи <<слегка>> сместятся всего
        лишь на несколько порядков в сторону помехи (смотри
        пункт~\ref{gng-u:7}). (вытягивание --- нормальный процесс,
        если помеха --- действительно одно из состояний системы)
      \item Следующим шагом будет (смотри пункт~\ref{gng-u:8}) расчёт
        фактора полезности: полезность также окажется очень большой.

        Проблемма: фактор полезности может слегка уменьшаться на
        каждом шаге (смотри пункт~\ref{gng-u:21}) а вот второго
        способа уменьшения фактора полезности в алгоритме нет.
        (Fixme: (нужно ли?) при создании нейрона для фактора
        полезности добавить пункт деления полезности (аналогичный
        пункту~\ref{gng-u:18} для локальной ошибки))
      \item Во время следующей эпохи расчёта обязательно будет удалён
        один из нейронов (смотри пункт~\ref{gng-u:11}). Причина:
        максимальная локальная ошибка $E_{max}$ на несколько порядков
        больше полезности любого нейрона $U_i$ (исключая конечно же
        нейрон выигравший помеху), а константа $k$ подобрана для
        обычного состояния сети и не рассчитана на помеху.
      \item Удаление в каждой эпохе будет продолжаться до тех пор пока
        не останется пара нейронов, ближайших к нейрону выигравшему
        помеху.
      \item Процесс добавления нейрона не будет успевать
        восстанавливать нейронную сеть: так как адаптационный шаг
        происходит только раз в $\lambda$ эпох (смотри
        пункт~\ref{gng-u:12}), а удаление случается в каждой эпохе.
      \end{itemize}

      Варианты решения проблеммы <<массового вымирания>>:
      \begin{itemize}
      \item Ничего не менять. Печально, но массовое вымирание --- один
        из этапов эволюции искусственной нейронной сети. Хорошо если
        все варианты помехи возникнут на ранней стадии формирования
        сети: потеряется не так много данных.
      \item Конечно можно решить эту проблему, просто игнорируя
        входные значения не укладывающиеся в предельно допустимые
        значения. Но что если эта помеха носит периодический характер
        и описывает ещё одно состояние системы? В таком случае
        игнорирование помехи --- плохой метод.
      \item Один из вариантов решения --- вместо максимального
        значения $E_{max}$ --- использовать медиану локальной ошибки
        $E_{median}$ (медианный фильтр достаточно прост и хорошо
        справляется с сильно неравномерными распределениями):

        $\frac{E_{median}}{U_i} > k$
      \item Другой способ --- (уже не константа) $k$ должна
        адаптироваться и к обычной сети и к возникновению
        помехи. Например рассчитываться как часть от среднего или
        медианы локальной ошибки.
      \end{itemize}
    \end{minipage}
  }

  Константа здесь играет решающее значение для способности отслеживать
  нестационарность: слишком большое ее значение приводит к более
  редким удалениям и, следовательно, снижению скорости
  адаптации. Слишком маленькое приводит к частому и многочисленному
  удалению не только действительно <<малополезных>>, но и других,
  вполне используемых нейронов.

\item \label{gng-u:12} \label{gng:11}
  Если номер текущей итерации кратен $\lambda$, и предельный
  размер сети не достигнут, создать новый нейрон $R$ по правилам. Со
  временем после нескольких циклов смещений накапливается информация,
  на основании которой принимается решение о месте, в котором должен
  быть добавлен новый нейрон. Этот процесс представляет собой
  коррекцию переменных ошибок всех нейронов слоя. Это необходимо для
  того, чтобы сеть <<забывала>> старые входные векторы и лучше
  реагировала на новые. Таким образом, достигается возможность
  использовать Расширяющийся нейронный газ для адаптации нейросети под
  нестационарные, а именно, медленно дрейфующие распределения входных
  сигналов.

\item \label{gng-u:13} \label{gng:12}
  Найти нейрон $U$ с наибольшей локальной ошибкой.

\item \label{gng-u:14} \label{gng:13}
  Среди соседей $U$ найти нейрон $V$ с максимальной ошибкой.

\item \label{gng-u:15} \label{gng:14}
  Создать узел $R$ <<посередине>> между $U$ и $V$:

  $\vec{W_r}=\frac{\vec{W_u} + \vec{W_v}}{2}$

\item \label{gng-u:16} \label{gng:15}
  Заменить связь между $U$ и $V$ на связи между $U$ и $R$, $R$ и
  $V$.

\item \label{gng-u:17} %\label{gng:-}
  при добавлении нового узла, его фактор полезности вычисляется как
  среднее арифметическое между полезностями соседних нейронов:

  $U_r \Leftarrow \frac{U_u + U_v}{2}$

\item \label{gng-u:18} \label{gng:16}
  Уменьшить ошибки нейронов $U$ и $V$, установить значение ошибки
  нейрона $R$.

  $E_u \Leftarrow E_u*a$
  
  $E_v \Leftarrow E_v*a$
  
  $E_r \Leftarrow E_u$

\item \label{gng-u:19} \label{gng:17}
  Большое значение этой ошибки служит указанием на то, что
  соответствующий нейрон лежит в области небольшого числа нейронов.

\item \label{gng-u:20} \label{gng:18}
  Уменьшить локальные ошибки всех нейронов на долю $\beta$
  
  $E_j \Leftarrow E_j - \beta*E_j$

\item \label{gng-u:21} % \label{gng:-}
  Уменьшить полезность всех нейронов на долю $\beta$

  $U_j \Leftarrow U_j - \beta*U_j$

\end{enumerate}


\section{Размышления о настройках}
Целочисленные обозначены так 10\\
числа с плавающей запятой (float) обозначены так 10.0

\begin{itemize}
\item EPS\_WINNER 0.1

  смещение победителя к вектору последних измерений

\item EPS\_NEIGHBOUR 0.01

  примерно в 10.0 раз меньше чем EPS\_WINNER = смещение соседей
  победителя к вектору последних измерений

\item EPS\_LOCAL\_ERROR 0.5

\item FACTOR\_BETA\_DECREASE\_LOCAL\_ERROR 0.0001

  уменьшение коэффициентов <<локальной ошибки>> и <<фактора
  полезности>> после каждого шага

\item LIMIT\_CONN\_AGE 20

  ограничение на возраст соединения

  Новый кластер создаётся когда нет событий между отдельными нейронами
  как следствие возраст соединения между этими нейронами увеличится
  (вот здесь есть тонкость: чтобы возраст увеличился он должен стать
  соседом выигравшего нейрона) и в какой-то момент превысит возраст
  превысит LIMIT\_CONN\_AGE --- соединение разорвётся.
\item K\_UTILITY 2.0

  Отвечает за адаптацию но относится к удалению мало полезных нейронов
  (возникает возможность создавать новые нейроны)

  Можно использовать разные значения: 0.1 1.0 10.0 100.0 1000.0
  большое значение - медленная адаптация; маленькое значение - быстрая
  адаптация но и быстрое забывание накопленных знаний.

\item LAMBDA\_STEP 60

  Отвечает за адаптацию: задаёт шаг в эпохах в которых
  возможно создание новых нейронов

  Если используются большие значения (больше 1000) то нейронная сеть
  медленно адаптируется, но при этом обучение присходит очень плавно
  --- так как не успевает реагировать на странные скачки

  Если выставить маленькие значения (меньше 10) то нейронная сеть
  быстро реагирует на помехи и выбросы и быстро обучается, но при этом
  может задействовать целую пачку нейронов на незначительную
  флуктуацию, а когда возникнет действительно новое состояние --- не
  сможет на него отреагировать (потому что исчерпает число нейронов
  ограниченное LIMIT\_NETWORK\_SIZE).  Нейронная сеть в итоге будет
  целую вечность перетягивать нейроны из той флуктуации в новое
  состояние.

  Значения около 100 (я ставил 60) нечто среднее.

\item LIMIT\_NETWORK\_SIZE 100

\item DIMENSION\_OF\_SENSOR 1128

\end{itemize}


\section{Форма структуры данных}

Исследователь может сам задавать форму структуры кластеров, будет ли
кластеризация выполнена для гиперсферы, гипертрубы или
гиперплоскости. Если он не обладает этими знаниями, то благодаря
значению собственной ковариационной матрицы можно определить
необходимую форму. Если структура имеет хотя бы одно собственное
значение меньше выбранного пользователем порога, то модель будет
гиперлинейной, в противном случае структуру необходимо рассматривать
как нелинейное многообразие. Дальнейшая проверка покажет, имеет ли
модель форму сферы или трубы. Проверка на сферичность зависит от
выполнения неравенства $np/na > \psi$, где $np$ — это количество
векторов внутри скопления, которое находится с помощью теоремы Жордана
Брауера, а $ap$ — площадь поверхности скопления и $\psi$ — заданный
пользователем порог. Если это неравенство приобретает форму
$np/na < \psi$, то формой кластера будет <<гипертруба>>.

\section{Расстояние от вектора Х до нейронов в кластерах разной формы}

\begin{itemize}
\item Для кластера в виде гипертрубы рассчитывается радиальная мера
  расстояния:

  $d^2_{i,j} = \frac
  {(\left\|\vec{x^t_i}-\vec{v_j}\right\|A_j - 1)^2 \left\|\vec{x^t_i}-\vec{v_j}\right\|^2}
  {\left\|\vec{x^t_i}-\vec{v_j}\right\|^2A_j}$
  
  где: \begin{itemize}
  \item $A_j$ — это положительная, определённой матрица, посчитанная
    для учёта эксцентриситета и ориентации гипертрубы. Значение $A_j$ для
    этого уравнения находится с помощью гиперлипсоида Лоунера, используя
    алгоритм Хачияна.
  \end{itemize}

\item Для определения расстояний в гиперплоскости следует использовать
  следующую формулу

  $d^2_{i,j} = 
  \left\|\vec{x^t_i}-\vec{v_j}\right\|^2A -
  \sum_{k=1}^{p} {<\vec{x^t_i}-\vec{v_j}, \vec{b_{j,k}}>^2A}$

  \todo{неясный верхний предел суммы}
  \todo{странные угловые скобочки что-то символизируют?}
  
  где: \begin{itemize}
  \item $A_j$, это сколь угодно позитивно определённая симметричная
    матрица весов. \todo{в формуле фигурирует просто $A$ без инедксов}
  \item $b_{j, k}$ оценивается с помощью нахождения собственных
    векторов нейронных узлов модели.
    \end{itemize}

  \item Для определения расстояния в гиперсфере необходимо
    использовать формулу
    
    $d^2_{i,j} = \left\|\vec{x^t_i}-\vec{v_j}\right\|^2$
    
    где: \begin{itemize}
    \item $w_i$ — либо среднее значение векторов, заключённых в
      плоскости. \todo{$w_i$ отсутствует в приведённой формуле}
    \end{itemize}
\end{itemize}


\section{Выводы в картинках}
\subsection{Объединение и разъединение кластеров}
Связь между нейронами разрывается, если длительное время между этими
нейронами не происходило событий (вектор входных данных не наблюдался
в непосредственной близости от соединения этих нейронов и возраст
соединения превысил ограничение). Рассмотрим это утверждение
подробнее:

В алгоритме <<возраст соединения>> или просто <<соединение>>
используется в следующих пунктах:
\begin{itemize}
\item Всё начинается с двух соединённых нейронов: пункт~№~\ref{gng-u:2} ---
  этот пункт описывает только первоначальный шаг
\item смещение топологических соседей имеющих соединения: пункт~№~\ref{gng-u:7}
  --- нейроны просто подтягиваются в сторону входного вектора
\item увеличение возраста соединения: пункт~№~\ref{gng-u:9}
\item обнуление возраста соединения: пункт~№~\ref{gng-u:10}
\item удаление соединения: пункт~№~\ref{gng-u:11}
\end{itemize}
Итого: важны для рассмотрения процесса объединения--разъединения
только пункты: \ref{gng-u:9}, \ref{gng-u:10} и \ref{gng-u:11}.

Напрямую на соединение влияет константа\\
$LIMIT\_CONN\_AGE = 15$

Если у соединения возраст превысит это число, то есть возраст
соединения станет равен 16 --- то соединение разорвётся.

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{01.eps} % fixme: strange error with path "../img/01.eps"
  \caption{два разных кластера; возраст всех соединений равен 0}
  \label{conn-step:01}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{02.eps}
  \caption{возникло состояние входного вектора <<c>> между двумя
    кластерами и как следствие: увеличился на 1 возраст всех
    соединений из первого победителя <<k>>; возникло соединения между
    первым победителем <<k>> и вторым победителем <<h>> с возрастом
    0. Два кластера объединились в один кластер}
  \label{conn-step:02}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{03.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<h>> (в том числе и возраст соединения k--h);
    обнулился возраст соединения между первым победителем <<h>> и
    вторым победителем <<d>>}
  \label{conn-step:03}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{04.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<d>>; обнулился возраст соединения между первым
    победителем <<d>> и вторым победителем <<e>>}
  \label{conn-step:04}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{05.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<e>>; обнулился возраст соединения между первым
    победителем <<e>> и вторым победителем <<f>>}
  \label{conn-step:05}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{06.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<f>>; обнулился возраст соединения между первым
    победителем <<f>> и вторым победителем <<g>>}
  \label{conn-step:06}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{07.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<g>>; обнулился возраст соединения между первым
    победителем <<g>> и вторым победителем <<h>>}
  \label{conn-step:07}
\end{figure}

\clearpage
По понятным причинам я пропущу соединение k--h иначе оно
обнулится. Продолжим \ldots

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{08.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<k>> (в том числе и возраст соединения k--h);
    обнулился возраст соединения между первым победителем <<k>> и
    вторым победителем <<l>>}
  \label{conn-step:08}
\end{figure}

\clearpage
К этому моменту уже понятно что:
\begin{enumerate}
\item события между кластерами вызывает их объединение и обнуление
  возраста соединения (Рис.~\ref{conn-step:02})
\item события внутри кластера делятся на те которые никак не влияют
  на соединение k--h (Рис.~\ref{conn-step:04}, \ref{conn-step:05},
  \ref{conn-step:06}, \ref{conn-step:07} и \ref{conn-step:09})
\item события внутри кластера, которые влияют на соединение k--h:
  \begin{enumerate}
  \item выиграл <<h>> и второй выигравший был <<d>>
    (Рис.~\ref{conn-step:03}) или <<g>> (Рис.~\ref{conn-step:10})
  \item выиграл <<k>> и второй выигравший был <<l>>
    (Рис.~\ref{conn-step:08}) или <<n>>
  \end{enumerate}
  только эти события увеличивают возраст соединения k--h и
  приближают разъединение кластеров.
  
  Причём важно, что эти события могут быть не последовательны: то есть
  тысячу раз входной вектор болтался внутри кластера (обходя стороной
  соединение k--h) и ни как не влиял на соединение k--h, потом подошёл
  ближе (например h--g) и увеличил возраст соединения на 1. После
  этого входной вектор снова может гулять внутри кластера тысячи эпох,
  подойти близко (h--d) увеличить на 1 возраст соединения k--h, и так
  далее \ldots

  События могут происходить и так: некие процессы в объекте вызвали
  многократное (как минимум 15 раз для данного примера) возникновение
  событий вблизи тонкого моста k--h и потом эти процессы взяли паузу
  на год. И вот спустя год слабая флуктуация попала в соединение k--n
  и разорвала соединение k--h увеличив возраст соединения до
  предела. Основные события приведшие к увеличению возраста соединения
  произошли год назад и остались незамеченными.

\item Следовательно, из того что кластера разъединились неправильно
  делать вывод о том, что произошло какое-то событие (если оно и
  произошло, то оно началось, для данного примера, как минимум 15 эпох
  назад).
\item Из разделения кластеров можно сделать один вывод: что в течении
  как минимум 15 эпох (опять же для данного примера) не происходило ни
  одного события между будущими кластерами.
\end{enumerate}



\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{09.eps}
  \caption{Пропустим тысячу шагов \ldots Увеличился на 1 возраст всех
    соединений из первого победителя <<f>>; обнулился возраст
    соединения между первым победителем <<f>> и вторым победителем
    <<d>>; Примечательно, что возраст соединения k--h уже 15}
  \label{conn-step:09}
\end{figure}

\begin{figure}[h]
  \center
  \includegraphics[width=5cm]{10.eps}
  \caption{увеличился на 1 возраст всех соединений из первого
    победителя <<h>> (при этом возраст соединения k--h стал 16 и при
    выполнении алгоритма пункт~№~\ref{gng-u:11} соединение обрывается,
    что и показано на рисунке красным пунктиром); обнулился возраст
    соединения между первым победителем <<h>> и вторым победителем
    <<g>>; Произошло разъединение кластеров}
  \label{conn-step:10}
\end{figure}

\clearpage

Подведём итог: назовём <<мостиком>> (возможно единственное) соединение
двух групп нейронов, пока ещё образующих кластер. Условие, при котором
<<мостик>> накапливает <<возраст соединения>> таково: вектор входных
данных должен наблюдаться в приграничной области внутри одной из групп
между:
\begin{itemize}
\item узлом, принадлежащим этому <<мостику>> (и это должен быть первый
  победитель)
\item и узлом (это соответственно второй победитель) в группе нейронов
  по ту же сторону от <<мостика>>, что и первый победитель
\end{itemize}
Любое событие возникающие посреди <<мостика>> только укрепляет его
(сбрасывает возраст до нуля).

Вывод всё тот же:

Связь между нейронами разрывается, если длительное время между этими
нейронами не происходило событий. Вектор входных данных не наблюдался
в непосредственной близости от соединения нейронов и возраст
соединения превысил ограничение.

Можно продолжить ту же мысль и для кластеров:

Связь между кластерами разрывается если длительное время между этими
кластерами не происходило событий. Каждый кластер соответствует
определённому состоянию изучаемого объекта. Момент образования нового
кластера говорит только о том что с этого момента нейронная сеть
различает эти два состояния объекта (никаких особенных событий в этот
момент может и не происходить).


\newpage
\bibliographystyle{plain}
\bibliography{algorithm}

\end{document}
